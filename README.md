# ğŸ§  vLLM Multi-Concurrent Chatbot with FastAPI UI

This project is a lightweight, production-ready chatbot interface powered by the [vLLM inference engine](https://github.com/vllm-project/vllm). It supports multi-concurrent generation and runs the backend and frontend as separate Docker containers.

---

## ğŸ“¦ Project Structure

```
.
â”œâ”€â”€ backend/           # vLLM backend for fast LLM inference
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ ui/                # FastAPI-based UI service
â”‚   â””â”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â””â”€â”€ README.md
```

## ğŸš€ Features

- ğŸ” Multi-concurrency support with vLLM engine  
- âš¡ GPU acceleration (if available)  
- ğŸ”Œ Modular: UI and backend run as separate containers  
- ğŸ³ Dockerized for local or production use  

## ğŸ› ï¸ Requirements

- Docker + Docker Compose  
- Optional: NVIDIA GPU with drivers (`nvidia-smi` working)  
- (For GPU use) NVIDIA Container Toolkit  

## ğŸ§ª Quickstart with Docker Compose

Clone the repo:

```bash
git clone https://github.com/your-org/Simple_LLM_ChatBot_using_vLLM.git
cd Simple_LLM_ChatBot_using_vLLM
```

Update the model path (if needed) in `docker-compose.yml`:

```yaml
volumes:
  - ./models:/app/models
```

Launch both UI and backend:

```bash
docker compose up --build
```

Access the chatbot UI:

Open your browser and go to http://localhost:7860

## ğŸ³ Docker Compose Overview

```yaml
services:
  vllm-backend:
    image: chinmay555/vllm:latest
    ports:
      - 8000:8000

  fastapi-ui:
    image: chinmay555/chat-ui:latest
    ports:
      - 7860:7860
```

## âš™ï¸ Environment Variables

You can configure the following via `.env` or hardcode in `docker-compose.yml`:

- `MODEL_PATH`: Path to your HF model (e.g., TinyLlama/TinyLlama-1.1B-Chat)  
- `PORT`: Backend port (default: 8000)  

## ğŸ§‘â€ğŸ’» API Usage

The FastAPI backend serves standard endpoints like:

```bash
POST /generate
GET  /health
```

To test:

```bash
curl -X POST http://localhost:8000/generate -d '{"prompt": "Tell me a joke"}'
```

## â˜¸ï¸ (Optional) Kubernetes Deployment

If you'd like to run this stack on Kubernetes instead of Docker Compose, you can use the following files:

- `vllm-deployment.yaml`
- `ui-service.yaml`
- `gpu-operator.yaml` (for GPU support)

Use `kubectl apply -f` to deploy each one.

## ğŸ§¼ Cleanup

```bash
docker compose down
```

## ğŸ“œ License

MIT License. See `LICENSE`.

## ğŸ™‹â€â™€ï¸ Questions?

Feel free to open an issue or reach out at [your-email@example.com].
